{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8152a0f0",
   "metadata": {},
   "source": [
    "# Sales Category Prediction - Final Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a2426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import psutil\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from glob import glob\n",
    "import scipy.sparse\n",
    "\n",
    "# Feature processing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce\n",
    "\n",
    "# Models\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# Feature importance\n",
    "import shap\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf5da5",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015562fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Paths\n",
    "    \"splits_dir\": \"../data/splits/\",\n",
    "    \"weather_path\": \"../data/weather_new.csv\",\n",
    "    \"artifacts_dir\": \"../artifacts/weather_simple/\",\n",
    "    \n",
    "    # Preprocessing\n",
    "    \"chunk_months\": 1,  # Size of sliding window in months (1 or 2)\n",
    "    \"drop_columns\": {\"group\", \"year\", \"unit\"},\n",
    "    \n",
    "    # Models\n",
    "    \"target_column\": \"qnt\",\n",
    "    \"date_column\": \"calday\",\n",
    "    \n",
    "    # Random seed\n",
    "    \"random_seed\": 42,\n",
    "    \n",
    "    # Optuna optimization settings\n",
    "    \"n_trials\": 50,        # Reduced from 50 to speed up optimization\n",
    "    \"use_pruning\": True,   # Enable pruning for faster convergence\n",
    "    \"early_stop\": 50,     # Aggressive early stopping for trials\n",
    "    \"timeout_per_trial\": 1800\n",
    "}\n",
    "\n",
    "# Create artifacts directory if it doesn't exist\n",
    "os.makedirs(CONFIG[\"artifacts_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01043b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature categories\n",
    "BOOL_EXPLICIT = {\"bu_exists\", \"freezing_day\", \"cold_day\", \"warm_day\", \"hot_day\"}\n",
    "BOOL_PATTERNS = [r\"^is_\", r\"^has_\", r\"_had_high_\", r\"_had_low_\", r\"_exists$\"]\n",
    "\n",
    "LOW_EXPLICIT = {\n",
    "    \"matrix_type\", \"country_id\", \"format_merch\", \"geolocal_type\",\n",
    "    \"season\", \"type_bonus_id\", \"seasonal_group\",\n",
    "    \"category_major\", \"category_detailed\", \"category_full\",\n",
    "    \"preciptype\", \"conditions\"\n",
    "}\n",
    "LOW_PATTERNS = [r\"^category_\"]\n",
    "\n",
    "MED_EXPLICIT = {\n",
    "    \"brand_id\", \"index_material\", \"index_store\", \"type_for_customer\",\n",
    "    \"week_iso\", \"day_of_week\", \"day_of_month\", \"month\", \"quarter\", \"source_month\"\n",
    "}\n",
    "\n",
    "NUM_PATTERNS = [\n",
    "    r\"^(temp|min|max|tempmax|tempmin|feelslike[a-z]*|dew|humidity|precip|snow|snowdepth\"\n",
    "    r\"|wind(gust|speed|dir)|sealevelpressure|cloudcover|visibility\"\n",
    "    r\"|solarradiation|solarenergy|uvindex|moonphase|daylight_hours\"\n",
    "    r\"|heat_index|temp_range)\",\n",
    "    r\".*_lag_\\\\d+d$\", r\".*_d\\\\d+to\\\\d+_(mean|min|max|std)$\"\n",
    "]\n",
    "\n",
    "# Define weather feature patterns for filtering baseline model - much more comprehensive list\n",
    "WEATHER_PATTERNS = [\n",
    "    # Base weather measurements\n",
    "    # r\"temp\", r\"feelslike\", r\"dew\", r\"humidity\", r\"precip\", \n",
    "    # r\"snow\", r\"wind\", r\"sealevelpressure\", r\"cloudcover\", r\"visibility\", \n",
    "    # r\"solar\", r\"uvindex\", r\"moonphase\", r\"daylight\", r\"heat_index\",\n",
    "    \n",
    "    # # Weather condition indicators\n",
    "    # r\"precipprob\", r\"precipcover\", r\"snowdepth\",\n",
    "    \n",
    "    # # Boolean weather flags\n",
    "    # r\"^has_precipitation\", r\"^has_rain\", r\"^has_snow\",\n",
    "    # r\"^is_clear\", r\"^is_overcast\", r\"^is_rain\", r\"^is_snow\", \n",
    "    # r\"^is_fog\", r\"^is_cloudy\", r\"^is_partially_cloudy\",\n",
    "    \n",
    "    # # Temperature categories\n",
    "    # r\"temp_range\", r\"freezing_day\", r\"cold_day\", r\"warm_day\", r\"hot_day\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7ae3a9",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f777a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / 1024 / 1024\n",
    "\n",
    "def log_step(step_name):\n",
    "    \"\"\"Log step name and memory usage\"\"\"\n",
    "    mem_mb = get_memory_usage()\n",
    "    logging.info(f\"{step_name}: {mem_mb:.2f} MB\")\n",
    "    return mem_mb\n",
    "\n",
    "# Evaluation metrics\n",
    "def calculate_mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    mask_nonzero = (y_true != 0) & (y_pred != 0)\n",
    "    mask_one_zero = ((y_true == 0) & (y_pred != 0)) | ((y_true != 0) & (y_pred == 0))\n",
    "    \n",
    "    mape_nonzero = np.abs((y_true[mask_nonzero] - y_pred[mask_nonzero]) / y_true[mask_nonzero])\n",
    "    mape_one_zero = np.ones(mask_one_zero.sum())\n",
    "    \n",
    "    if len(mape_nonzero) + len(mape_one_zero) == 0:\n",
    "        return 0\n",
    "    \n",
    "    total_mape = np.concatenate([mape_nonzero, mape_one_zero]) if len(mape_one_zero) > 0 and len(mape_nonzero) > 0 else \\\n",
    "                (mape_nonzero if len(mape_nonzero) > 0 else mape_one_zero)\n",
    "    return np.mean(total_mape)\n",
    "\n",
    "def calculate_smape(y_true, y_pred):\n",
    "    mask_nonzero = (y_true != 0) & (y_pred != 0)\n",
    "    mask_one_zero = ((y_true == 0) & (y_pred != 0)) | ((y_true != 0) & (y_pred == 0))\n",
    "    \n",
    "    smape_nonzero = 2 * np.abs(y_true[mask_nonzero] - y_pred[mask_nonzero]) / (np.abs(y_true[mask_nonzero]) + np.abs(y_pred[mask_nonzero]))\n",
    "    smape_one_zero = np.ones(mask_one_zero.sum())\n",
    "    \n",
    "    if len(smape_nonzero) + len(smape_one_zero) == 0:\n",
    "        return 0\n",
    "    \n",
    "    total_smape = np.concatenate([smape_nonzero, smape_one_zero]) if len(smape_one_zero) > 0 and len(smape_nonzero) > 0 else \\\n",
    "                 (smape_nonzero if len(smape_nonzero) > 0 else smape_one_zero)\n",
    "    return np.mean(total_smape)\n",
    "\n",
    "# Additional metrics\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    \"\"\"Calculate Root Mean Squared Error\"\"\"\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "def calculate_r2(y_true, y_pred):\n",
    "    \"\"\"Calculate R-squared (coefficient of determination)\"\"\"\n",
    "    from sklearn.metrics import r2_score\n",
    "    return r2_score(y_true, y_pred)\n",
    "\n",
    "def calculate_wape(y_true, y_pred):\n",
    "    \"\"\"Calculate Weighted Absolute Percentage Error\"\"\"\n",
    "    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))\n",
    "\n",
    "def calculate_bias(y_true, y_pred):\n",
    "    \"\"\"Calculate bias (Mean Percent Forecast Error)\n",
    "    >0 → over-forecast (over-stock)\n",
    "    <0 → under-forecast (under-stock)\"\"\"\n",
    "    mask = y_true != 0\n",
    "    if not np.any(mask):\n",
    "        return 0\n",
    "    \n",
    "    percent_errors = (y_pred[mask] - y_true[mask]) / y_true[mask]\n",
    "    return np.mean(percent_errors)\n",
    "\n",
    "def calculate_stock_stats(y_true, y_pred, action_price):\n",
    "    \"\"\"Calculate over-stock and under-stock statistics\"\"\"\n",
    "    differences = y_pred - y_true\n",
    "    over_stock = np.where(differences > 0, differences * action_price, 0)\n",
    "    under_stock = np.where(differences < 0, -differences * action_price, 0)\n",
    "    \n",
    "    total_sales = np.sum(y_true * action_price)\n",
    "    over_stock_sum = np.sum(over_stock)\n",
    "    under_stock_sum = np.sum(under_stock)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if total_sales == 0:\n",
    "        over_stock_percent = 0\n",
    "        under_stock_percent = 0\n",
    "    else:\n",
    "        over_stock_percent = over_stock_sum / total_sales * 100\n",
    "        under_stock_percent = under_stock_sum / total_sales * 100\n",
    "    \n",
    "    return {\n",
    "        'total_sales': total_sales,\n",
    "        'over_stock': over_stock_sum,\n",
    "        'under_stock': under_stock_sum,\n",
    "        'over_stock_percent': over_stock_percent,\n",
    "        'under_stock_percent': under_stock_percent\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be495ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_predictions(y_pred, qnt_max):\n",
    "    \"\"\"Apply post-processing rules to prediction\"\"\"\n",
    "    # Clip negative values to 0\n",
    "    y_pred = np.clip(y_pred, 0, None)\n",
    "    \n",
    "    # Apply outlier correction where needed\n",
    "    mask = (qnt_max >= 5) & (y_pred > 2 * qnt_max)\n",
    "    if np.any(mask):\n",
    "        y_pred[mask] = 2 * qnt_max[mask]\n",
    "    \n",
    "    # Round to nearest integer\n",
    "    y_pred = np.round(y_pred).astype(int)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40416f5e",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8b8144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_types(df):\n",
    "    \"\"\"Identify feature types based on rules\"\"\"\n",
    "    columns = df.columns.tolist()\n",
    "    feature_types = {'bool_cols': [], 'low_cat_cols': [], 'med_cat_cols': [], 'num_cols': []}\n",
    "    \n",
    "    # Find target and ID columns\n",
    "    target_col = CONFIG['target_column']\n",
    "    date_col = CONFIG['date_column']\n",
    "    drop_cols = list(CONFIG['drop_columns']) + [target_col, date_col] if target_col in columns and date_col in columns else []\n",
    "    \n",
    "    # Collect columns by type\n",
    "    for col in columns:\n",
    "        if col in drop_cols:\n",
    "            continue\n",
    "        \n",
    "        # Check bool columns\n",
    "        if col in BOOL_EXPLICIT or any(re.match(pattern, col) for pattern in BOOL_PATTERNS) or df[col].dtype == bool:\n",
    "            feature_types['bool_cols'].append(col)\n",
    "            continue\n",
    "            \n",
    "        # Check low cardinality categorical\n",
    "        if col in LOW_EXPLICIT or any(re.match(pattern, col) for pattern in LOW_PATTERNS):\n",
    "            feature_types['low_cat_cols'].append(col)\n",
    "            continue\n",
    "            \n",
    "        # Check medium cardinality categorical\n",
    "        if col in MED_EXPLICIT:\n",
    "            feature_types['med_cat_cols'].append(col)\n",
    "            continue\n",
    "            \n",
    "        # Check numeric by pattern\n",
    "        if any(re.match(pattern, col) for pattern in NUM_PATTERNS) or df[col].dtype in ['int32', 'int64', 'float32', 'float64']:\n",
    "            feature_types['num_cols'].append(col)\n",
    "    \n",
    "    return feature_types\n",
    "\n",
    "def create_column_transformer(df):\n",
    "    \"\"\"Create a ColumnTransformer for preprocessing\"\"\"\n",
    "    feature_types = get_feature_types(df)\n",
    "    \n",
    "    # Log the number of features of each type for debugging\n",
    "    logging.info(f\"Feature counts: bool={len(feature_types['bool_cols'])}, low_cat={len(feature_types['low_cat_cols'])}, \"\n",
    "                 f\"med_cat={len(feature_types['med_cat_cols'])}, num={len(feature_types['num_cols'])}\")\n",
    "    \n",
    "    transformers = []\n",
    "    \n",
    "    # Boolean columns to uint8\n",
    "    if feature_types['bool_cols']:\n",
    "        transformers.append(('bool', 'passthrough', feature_types['bool_cols']))\n",
    "    \n",
    "    # Low cardinality categorical columns to one-hot\n",
    "    if feature_types['low_cat_cols']:\n",
    "        transformers.append(('low_cat', \n",
    "                            OneHotEncoder(sparse_output=True, handle_unknown='ignore', dtype=np.int8),\n",
    "                            feature_types['low_cat_cols']))\n",
    "    \n",
    "    # Medium cardinality categorical columns with count encoding\n",
    "    if feature_types['med_cat_cols']:\n",
    "        # First check if we have any medium cardinality columns\n",
    "        if len(feature_types['med_cat_cols']) > 0:\n",
    "            # Create a copy of the DataFrame with only the medium cardinality columns\n",
    "            med_cat_df = df[feature_types['med_cat_cols']].copy()\n",
    "            \n",
    "            # Convert all columns to categorical type\n",
    "            for col in med_cat_df.columns:\n",
    "                med_cat_df[col] = med_cat_df[col].astype('category')\n",
    "            transformers.append(('med_cat', \n",
    "                              ce.CountEncoder(normalize=True), \n",
    "                              feature_types['med_cat_cols']))\n",
    "    \n",
    "    # Numeric columns - just pass through, no imputation needed\n",
    "    if feature_types['num_cols']:\n",
    "        transformers.append(('num', \n",
    "                            'passthrough', \n",
    "                            feature_types['num_cols']))\n",
    "    \n",
    "    return ColumnTransformer(transformers, remainder='drop', n_jobs=1)  # Use single thread to reduce memory\n",
    "\n",
    "def cast_types(df):\n",
    "    \"\"\"Downcast types for efficiency\"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "        elif df[col].dtype == 'int64':\n",
    "            df[col] = df[col].astype('int32')\n",
    "        elif df[col].dtype == 'bool':\n",
    "            df[col] = df[col].astype('uint8')\n",
    "    return df\n",
    "    \n",
    "def load_and_preprocess_data(split_file):\n",
    "    \"\"\"Load and preprocess a split file\"\"\"\n",
    "    log_step(f\"Loading split file {os.path.basename(split_file)}\")\n",
    "    \n",
    "    # Read the split file\n",
    "    df = pd.read_csv(split_file)\n",
    "    \n",
    "    # Read weather data\n",
    "    weather_df = pd.read_csv(CONFIG['weather_path'])\n",
    "    \n",
    "    # Merge with weather data\n",
    "    df = pd.merge(df, weather_df, on=CONFIG['date_column'], how='left')\n",
    "    \n",
    "    # Cast types\n",
    "    df = cast_types(df)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    for col in CONFIG['drop_columns']:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    # Sort by date\n",
    "    df.sort_values(by=CONFIG['date_column'], inplace=True)\n",
    "    \n",
    "    # Convert date to datetime\n",
    "    if df[CONFIG['date_column']].dtype == 'object':\n",
    "        df[CONFIG['date_column']] = pd.to_datetime(df[CONFIG['date_column']])\n",
    "    \n",
    "    # Explicitly convert medium cardinality columns to category without logging each conversion\n",
    "    for col in MED_EXPLICIT:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    # Calculate max_qnt for post-processing only if it doesn't exist\n",
    "    if 'qnt_max' not in df.columns:\n",
    "        logging.info(f\"'qnt_max' column not found, calculating it now\")\n",
    "        df['qnt_max'] = df.groupby(['brand_id', 'country_id', 'category_detailed'])[CONFIG['target_column']].transform('max')\n",
    "    \n",
    "    # Log transformation for specific quantity columns\n",
    "    log_columns = [\n",
    "        'qnt', 'qnt_loss', 'qnt_lag_14d', 'qnt_lag_21d', 'qnt_lag_28d', \n",
    "        'qnt_lag_avg', 'qnt_max', 'qnt_min', 'qnt_mean', 'qnt_median'\n",
    "    ]\n",
    "    \n",
    "    # Store original values before transformation\n",
    "    for col in log_columns:\n",
    "        if col in df.columns:\n",
    "            df[f\"{col}_orig\"] = df[col].copy()\n",
    "    \n",
    "    # Apply log1p transformation\n",
    "    for col in log_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = np.log1p(df[col])\n",
    "    \n",
    "    log_step(f\"Preprocessed split shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def filter_weather_features(feature_cols):\n",
    "    \"\"\"Filter out weather-related features for baseline model\"\"\"\n",
    "    baseline_features = []\n",
    "    weather_features = []\n",
    "\n",
    "    WEATHER_COLUMNS = [\n",
    "        \"tempmax\", \"tempmin\", \"temp\", \"feelslikemax\", \"feelslikemin\", \"feelslike\",\n",
    "        \"dew\", \"humidity\", \"precip\", \"precipprob\", \"precipcover\", \"snow\", \"snowdepth\",\n",
    "        \"windgust\", \"windspeed\", \"winddir\", \"sealevelpressure\", \"cloudcover\", \"visibility\",\n",
    "        \"solarradiation\", \"solarenergy\", \"uvindex\", \"moonphase\", \"daylight_hours\"\n",
    "    ]\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        if col in WEATHER_COLUMNS:\n",
    "            weather_features.append(col)\n",
    "        else:\n",
    "            baseline_features.append(col)\n",
    "            \n",
    "    logging.info(f\"Total features: {len(feature_cols)}, Baseline features: {len(baseline_features)}, Weather features: {len(weather_features)}\")\n",
    "    \n",
    "    # Removed detailed weather feature logging that was too verbose\n",
    "    \n",
    "    return baseline_features, weather_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba85451",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef08a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X_train, y_train, X_valid, y_valid):\n",
    "    \"\"\"Define the objective function for Optuna\"\"\"\n",
    "    param = {\n",
    "        # Removed 'objective' parameter since it's the default\n",
    "        'metric': 'rmse',  # Changed from 'mae' to 'rmse' which is better for log-transformed data\n",
    "        'device_type': 'gpu',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'gpu_platform_id': 1,\n",
    "        'gpu_device_id': 0,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': CONFIG[\"random_seed\"],\n",
    "        'n_estimators': 500,\n",
    "        'verbose': -1,\n",
    "                    \n",
    "        # Hyperparameters to optimize\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 63, 255),\n",
    "        'max_depth': trial.suggest_int('max_depth', 6, 18),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 250),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1e-5, 5, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 2.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 3.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'max_bin': trial.suggest_categorical('max_bin', [255]),\n",
    "    }\n",
    "    \n",
    "    # Initialize LightGBM model with the trial parameters\n",
    "    model = LGBMRegressor(**param)\n",
    "    \n",
    "    # Import lightgbm for callbacks\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "    # Custom callback for Hyperband pruning\n",
    "    pruning_callback = optuna.integration.LightGBMPruningCallback(\n",
    "        trial, 'rmse', valid_name='valid_0'\n",
    "    )\n",
    "    \n",
    "    # Train the model with early stopping and pruning callback\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=CONFIG[\"early_stop\"]),\n",
    "            lgb.log_evaluation(period=100),\n",
    "            pruning_callback  # Add Optuna pruning callback\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Explicitly release memory\n",
    "    result = model.best_score_['valid_0']['rmse']\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "    # Return the best validation score\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(X_train, y_train, X_valid, y_valid, split_name):\n",
    "    \"\"\"Run Optuna optimization to find best hyperparameters\"\"\"\n",
    "    log_step(f\"Starting hyperparameter optimization for {split_name}\")\n",
    "    \n",
    "    # Define path for saved hyperparameters\n",
    "    params_file = f\"{CONFIG['artifacts_dir']}best_params_{split_name}.json\"\n",
    "    \n",
    "    # Check if we already have saved parameters\n",
    "    if os.path.exists(params_file):\n",
    "        import json\n",
    "        logging.info(f\"Found saved hyperparameters for {split_name}, loading from {params_file}\")\n",
    "        with open(params_file, 'r') as f:\n",
    "            loaded_params = json.load(f)\n",
    "            \n",
    "        # Convert parameters to the correct types\n",
    "        best_params = {}\n",
    "        \n",
    "        # Integer parameters\n",
    "        int_params = ['num_leaves', 'max_depth', 'min_child_samples']\n",
    "        for param in int_params:\n",
    "            if param in loaded_params:\n",
    "                best_params[param] = int(loaded_params[param])\n",
    "        \n",
    "        # Float parameters\n",
    "        float_params = ['learning_rate', 'subsample', 'colsample_bytree', 'reg_alpha', 'reg_lambda']\n",
    "        for param in float_params:\n",
    "            if param in loaded_params:\n",
    "                best_params[param] = float(loaded_params[param])\n",
    "        \n",
    "        # Add fixed parameters that may not be saved in the JSON\n",
    "        best_params.update({\n",
    "            # Removed 'objective': 'regression'\n",
    "            'metric': 'rmse',  # Changed from 'mae' to 'rmse'\n",
    "            'boosting_type': 'gbdt',\n",
    "            'device_type': 'gpu',\n",
    "            'gpu_platform_id': 1,\n",
    "            'gpu_device_id': 0,\n",
    "            'n_jobs': 1,\n",
    "            'random_state': CONFIG[\"random_seed\"],\n",
    "            'n_estimators': 20000,  # Will use early stopping\n",
    "            'verbose': -1,  # Suppress verbose output\n",
    "        })\n",
    "        \n",
    "        logging.info(f\"Loaded hyperparameters: {best_params}\")\n",
    "        return best_params\n",
    "    \n",
    "    # Use Hyperband pruner for more efficient hyperparameter search\n",
    "    from optuna.pruners import HyperbandPruner\n",
    "    \n",
    "    # Hyperband pruner is more efficient than MedianPruner for tree-based models\n",
    "    # It allocates resources adaptively, terminating unpromising trials early\n",
    "    hyperband_pruner = HyperbandPruner(\n",
    "        min_resource=10,     # Minimum number of iterations before pruning\n",
    "        max_resource=500,    # Maximum iterations per trial\n",
    "        reduction_factor=3    # Reduction factor for successive halving\n",
    "    )\n",
    "    \n",
    "    # Use a more sophisticated TPE sampler with Hyperband pruner\n",
    "    sampler = TPESampler(\n",
    "        seed=CONFIG[\"random_seed\"],\n",
    "        n_startup_trials=10,   \n",
    "        multivariate=True,     # Consider parameter correlations\n",
    "        consider_prior=True,   # Use prior distributions\n",
    "        consider_magic_clip=True,\n",
    "        consider_endpoints=True\n",
    "    )\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',  # Still minimize for RMSE\n",
    "        sampler=sampler,\n",
    "        pruner=hyperband_pruner,  # Use Hyperband pruner instead of MedianPruner\n",
    "        study_name=f\"lgbm_optimization_{split_name}\"\n",
    "    )\n",
    "    \n",
    "    # Ensure we're optimizing on a representative sample\n",
    "    logging.info(f\"Optimizing hyperparameters using full feature set (including weather)\")\n",
    "    \n",
    "    if X_train.shape[0] > 200000:\n",
    "        sample_indices = np.random.choice(X_train.shape[0], 200000, replace=False)\n",
    "        X_train_sample = X_train[sample_indices]\n",
    "        y_train_sample = y_train.iloc[sample_indices] if isinstance(y_train, pd.Series) else y_train[sample_indices]\n",
    "        \n",
    "        # Run optimization with progress bar on sampled data\n",
    "        study.optimize(\n",
    "            lambda trial: objective(trial, X_train_sample, y_train_sample, X_valid, y_valid),\n",
    "            n_trials=CONFIG[\"n_trials\"],\n",
    "            callbacks=[\n",
    "                lambda study, trial: tqdm.write(f\"Trial {trial.number} finished with value: {trial.value}\"),\n",
    "                # Add explicit GPU memory cleanup after each trial\n",
    "                lambda study, trial: gc.collect()\n",
    "            ],\n",
    "            show_progress_bar=True,\n",
    "            n_jobs=1  # Use sequential processing for GPU tasks\n",
    "        )\n",
    "    else:\n",
    "        # Run optimization with progress bar on full dataset for smaller datasets\n",
    "        study.optimize(\n",
    "            lambda trial: objective(trial, X_train, y_train, X_valid, y_valid),\n",
    "            n_trials=CONFIG[\"n_trials\"],\n",
    "            callbacks=[\n",
    "                lambda study, trial: tqdm.write(f\"Trial {trial.number} finished with value: {trial.value}\"),\n",
    "                # Add explicit GPU memory cleanup after each trial\n",
    "                lambda study, trial: gc.collect()\n",
    "            ],\n",
    "            show_progress_bar=True,\n",
    "            n_jobs=1  # Use sequential processing for GPU tasks\n",
    "        )\n",
    "\n",
    "    # Get the best parameters\n",
    "    best_params = study.best_params\n",
    "    best_value = study.best_value\n",
    "    \n",
    "    logging.info(f\"Best hyperparameters for {split_name}: {best_params}\")\n",
    "    logging.info(f\"Best validation MAE: {best_value}\")\n",
    "    \n",
    "    # Add fixed parameters\n",
    "    best_params.update({\n",
    "        # Removed 'objective': 'regression'\n",
    "        'metric': 'rmse',  # Changed from 'mae' to 'rmse'\n",
    "        'device_type': 'gpu',\n",
    "        'gpu_platform_id': 1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'gpu_device_id': 0,\n",
    "        'n_jobs': 1,\n",
    "        'random_state': CONFIG[\"random_seed\"],\n",
    "        'n_estimators': 20000,  # Use full iterations for final model\n",
    "        'verbose': -1,  # Suppress verbose output\n",
    "    })\n",
    "        \n",
    "            # Save the best parameters\n",
    "    import json\n",
    "    with open(params_file, 'w') as f:\n",
    "        # Save only the parameters that should be persisted (exclude fixed parameters)\n",
    "        params_to_save = {k: v for k, v in best_params.items() if k in study.best_params}\n",
    "        json.dump(params_to_save, f, indent=4)\n",
    "    logging.info(f\"Saved best hyperparameters to {params_file}\")\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c899217f",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ddc4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(df, split_name):\n",
    "    \"\"\"Train models on the entire split with optimized hyperparameters\"\"\"\n",
    "    log_step(f\"Starting model training for split {split_name}\")\n",
    "    \n",
    "    # Define path for split-specific metrics\n",
    "    metrics_file = f\"{CONFIG['artifacts_dir']}{split_name}_metrics.csv\"\n",
    "    \n",
    "    # Check if metrics file already exists\n",
    "    if os.path.exists(metrics_file):\n",
    "        logging.info(f\"Loading existing metrics for {split_name} from {metrics_file}\")\n",
    "        metrics_df = pd.read_csv(metrics_file)\n",
    "        \n",
    "        # Also check for SHAP file\n",
    "        shap_file = f\"{CONFIG['artifacts_dir']}{split_name}_feature_importance.csv\"\n",
    "        if os.path.exists(shap_file):\n",
    "            shap_df = pd.read_csv(shap_file)\n",
    "            return metrics_df, shap_df\n",
    "    \n",
    "    # Define target and date columns\n",
    "    target_col = CONFIG['target_column']  # 'qnt' (log-transformed)\n",
    "    target_orig_col = f\"{target_col}_orig\"  # Original target for evaluation\n",
    "    date_col = CONFIG['date_column']\n",
    "    qnt_max_col = 'qnt_max'\n",
    "    qnt_max_orig_col = 'qnt_max_orig'\n",
    "    \n",
    "    # Exclude target column and date column from features\n",
    "    all_feature_cols = [col for col in df.columns if col not in [target_col, target_orig_col, date_col] and not col.endswith('_orig')]\n",
    "    \n",
    "    # Filter features for baseline model\n",
    "    baseline_feature_cols, weather_features = filter_weather_features(all_feature_cols)\n",
    "    \n",
    "    # Verify weather features aren't in baseline (double-check)\n",
    "    logging.info(f\"Baseline model will use {len(baseline_feature_cols)} features (excluding {len(weather_features)} weather features)\")\n",
    "    \n",
    "    # Create preprocessors for both models\n",
    "    full_preprocessor = create_column_transformer(df[all_feature_cols])\n",
    "    baseline_preprocessor = create_column_transformer(df[baseline_feature_cols])\n",
    "    \n",
    "    # Create split for training (80%) and validation (20%)\n",
    "    df_sorted = df.sort_values(by=date_col)\n",
    "    split_idx = int(len(df_sorted) * 0.8)\n",
    "    \n",
    "    train_df = df_sorted.iloc[:split_idx]\n",
    "    valid_df = df_sorted.iloc[split_idx:]\n",
    "    \n",
    "    # Extract features and target\n",
    "    X_train_full = train_df[all_feature_cols]\n",
    "    X_train_baseline = train_df[baseline_feature_cols]\n",
    "    y_train = train_df[target_col]  # Log-transformed target\n",
    "    \n",
    "    X_valid_full = valid_df[all_feature_cols]\n",
    "    X_valid_baseline = valid_df[baseline_feature_cols]\n",
    "    y_valid = valid_df[target_col]  # Log-transformed target\n",
    "    y_valid_orig = valid_df[target_orig_col]  # Original target for evaluation\n",
    "    qnt_max_valid_orig = valid_df[qnt_max_orig_col]\n",
    "    \n",
    "    # Get action prices for stock statistics\n",
    "    action_price = valid_df['action_price'] if 'action_price' in valid_df.columns else np.ones(len(valid_df))\n",
    "    \n",
    "    log_step(f\"Train size: {len(y_train)}, Validation size: {len(y_valid)}\")\n",
    "    \n",
    "    # Preprocess features for full model\n",
    "    log_step(\"Preprocessing features for full model\")\n",
    "    X_train_full_proc = full_preprocessor.fit_transform(X_train_full)\n",
    "    X_valid_full_proc = full_preprocessor.transform(X_valid_full)\n",
    "    \n",
    "    # Optimize hyperparameters for full model\n",
    "    best_params = optimize_hyperparameters(X_train_full_proc, y_train, X_valid_full_proc, y_valid, split_name)\n",
    "    \n",
    "    # Clean up memory\n",
    "    del X_train_full\n",
    "    gc.collect()\n",
    "    log_step(\"After full model preprocessing\")\n",
    "    \n",
    "    # Preprocess features for baseline model\n",
    "    log_step(\"Preprocessing features for baseline model\")\n",
    "    X_train_baseline_proc = baseline_preprocessor.fit_transform(X_train_baseline)\n",
    "    X_valid_baseline_proc = baseline_preprocessor.transform(X_valid_baseline)\n",
    "    \n",
    "    # Clean up memory\n",
    "    del X_train_baseline\n",
    "    gc.collect()\n",
    "    log_step(\"After baseline preprocessing\")\n",
    "    \n",
    "    # Results dictionary with all metrics\n",
    "    results = {\n",
    "        \"split\": [],\n",
    "        \"model\": [],\n",
    "        \"MAE\": [],\n",
    "        \"MAPE\": [],\n",
    "        \"sMAPE\": [],\n",
    "        \"RMSE\": [],\n",
    "        \"R2\": [],\n",
    "        \"WAPE\": [],\n",
    "        \"Bias\": [],\n",
    "        \"over_stock\": [],\n",
    "        \"under_stock\": [],\n",
    "        \"over_stock_percent\": [],\n",
    "        \"under_stock_percent\": [],\n",
    "        \"total_sales\": [],\n",
    "        \"train_time_sec\": [],\n",
    "        \"rows_train\": [],\n",
    "        \"rows_valid\": []\n",
    "    }\n",
    "    \n",
    "    # Import lightgbm for callbacks\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "    # Define model types and their associated data\n",
    "    model_configs = {\n",
    "        \"model_with_weather\": {\n",
    "            \"file_path\": f\"{CONFIG['artifacts_dir']}{split_name}_model_with_weather.txt\",\n",
    "            \"X_train\": X_train_full_proc,\n",
    "            \"X_valid\": X_valid_full_proc,\n",
    "            \"preprocessed\": True\n",
    "        },\n",
    "        \"baseline_model\": {\n",
    "            \"file_path\": f\"{CONFIG['artifacts_dir']}{split_name}_baseline_model.txt\",\n",
    "            \"X_train\": X_train_baseline_proc,\n",
    "            \"X_valid\": X_valid_baseline_proc,\n",
    "            \"preprocessed\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Train or load each model\n",
    "    for model_type, config in model_configs.items():\n",
    "        model_file = config[\"file_path\"]\n",
    "        X_train_proc = config[\"X_train\"]\n",
    "        X_valid_proc = config[\"X_valid\"]\n",
    "        \n",
    "        if os.path.exists(model_file):\n",
    "            # Load existing model\n",
    "            logging.info(f\"Loading existing {model_type} from {model_file}\")\n",
    "            model = lgb.Booster(model_file=model_file)\n",
    "            train_time = 0  # No training time for loaded models\n",
    "        else:\n",
    "            # Train new model with optimized hyperparameters\n",
    "            logging.info(f\"Training new {model_type}\")\n",
    "            model_instance = LGBMRegressor(**best_params)\n",
    "            \n",
    "            # Set verbosity to -1 to suppress info messages and add GPU settings if not already present\n",
    "            if 'gpu_platform_id' not in best_params:\n",
    "                model_instance.set_params(gpu_platform_id=1, gpu_device_id=0, verbose=-1)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            model_instance.fit(\n",
    "                X_train_proc, y_train,\n",
    "                eval_set=[(X_valid_proc, y_valid)],\n",
    "                callbacks=[\n",
    "                    lgb.early_stopping(stopping_rounds=500),\n",
    "                    lgb.log_evaluation(period=250)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            train_time = time.time() - start_time\n",
    "            log_step(f\"{model_type} training completed in {train_time:.2f}s\")\n",
    "            \n",
    "            # Save the model\n",
    "            model_instance.booster_.save_model(model_file)\n",
    "            model = model_instance.booster_\n",
    "        \n",
    "        # Make predictions\n",
    "        if isinstance(model, lgb.Booster):\n",
    "            # For loaded Booster models\n",
    "            y_pred_log = model.predict(X_valid_proc)\n",
    "        else:\n",
    "            # For trained LGBMRegressor models\n",
    "            y_pred_log = model.predict(X_valid_proc)\n",
    "        \n",
    "        # Log prediction statistics\n",
    "        logging.info(f\"{model_type} - Log-scale predictions: min={y_pred_log.min():.4f}, max={y_pred_log.max():.4f}, mean={y_pred_log.mean():.4f}\")\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        y_pred = np.expm1(y_pred_log)\n",
    "        \n",
    "        # Post-process predictions\n",
    "        y_pred = postprocess_predictions(y_pred, qnt_max_valid_orig.values)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = calculate_mae(y_valid_orig.values, y_pred)\n",
    "        mape = calculate_mape(y_valid_orig.values, y_pred)\n",
    "        smape = calculate_smape(y_valid_orig.values, y_pred)\n",
    "        rmse = calculate_rmse(y_valid_orig.values, y_pred)\n",
    "        r2 = calculate_r2(y_valid_orig.values, y_pred)\n",
    "        wape = calculate_wape(y_valid_orig.values, y_pred)\n",
    "        bias = calculate_bias(y_valid_orig.values, y_pred)\n",
    "        \n",
    "        # Calculate stock statistics\n",
    "        stock_stats = calculate_stock_stats(y_valid_orig.values, y_pred, action_price)\n",
    "        \n",
    "        # Log detailed results\n",
    "        logging.info(f\"{model_type} - Training time: {train_time:.2f}s\")\n",
    "        logging.info(f\"{model_type} - MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "        logging.info(f\"{model_type} - MAPE: {mape:.4f}, sMAPE: {smape:.4f}, WAPE: {wape:.4f}, Bias: {bias:.4f}\")\n",
    "        logging.info(f\"{model_type} - Over-stock: {stock_stats['over_stock']:.2f} ({stock_stats['over_stock_percent']:.2f}%), Under-stock: {stock_stats['under_stock']:.2f} ({stock_stats['under_stock_percent']:.2f}%)\")\n",
    "        \n",
    "        # Save results\n",
    "        results[\"split\"].append(split_name)\n",
    "        results[\"model\"].append(model_type)\n",
    "        results[\"MAE\"].append(mae)\n",
    "        results[\"MAPE\"].append(mape)\n",
    "        results[\"sMAPE\"].append(smape)\n",
    "        results[\"RMSE\"].append(rmse)\n",
    "        results[\"R2\"].append(r2)\n",
    "        results[\"WAPE\"].append(wape)\n",
    "        results[\"Bias\"].append(bias)\n",
    "        results[\"over_stock\"].append(stock_stats['over_stock'])\n",
    "        results[\"under_stock\"].append(stock_stats['under_stock'])\n",
    "        results[\"over_stock_percent\"].append(stock_stats['over_stock_percent'])\n",
    "        results[\"under_stock_percent\"].append(stock_stats['under_stock_percent'])\n",
    "        results[\"total_sales\"].append(stock_stats['total_sales'])\n",
    "        results[\"train_time_sec\"].append(train_time)\n",
    "        results[\"rows_train\"].append(len(y_train))\n",
    "        results[\"rows_valid\"].append(len(y_valid))\n",
    "    \n",
    "    # Calculate SHAP values for feature importance\n",
    "    log_step(\"Calculating SHAP values for feature importance\")\n",
    "    \n",
    "    # Load the models again for SHAP calculation if needed\n",
    "    model_with_weather = None\n",
    "    baseline_model = None\n",
    "    \n",
    "    # Load or use model_with_weather\n",
    "    weather_model_file = f\"{CONFIG['artifacts_dir']}{split_name}_model_with_weather.txt\"\n",
    "    if os.path.exists(weather_model_file):\n",
    "        try:\n",
    "            model_with_weather = lgb.Booster(model_file=weather_model_file)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading model_with_weather for SHAP: {e}\")\n",
    "    \n",
    "    # Load or use baseline_model\n",
    "    baseline_model_file = f\"{CONFIG['artifacts_dir']}{split_name}_baseline_model.txt\"\n",
    "    if os.path.exists(baseline_model_file):\n",
    "        try:\n",
    "            baseline_model = lgb.Booster(model_file=baseline_model_file)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading baseline_model for SHAP: {e}\")\n",
    "    \n",
    "    # Skip SHAP calculation if models couldn't be loaded\n",
    "    if model_with_weather is None or baseline_model is None:\n",
    "        logging.warning(\"Skipping SHAP calculation due to missing models\")\n",
    "        feature_importance_data = pd.DataFrame({\n",
    "            \"split\": [], \"model_type\": [], \"feature\": [], \"importance_pct\": []\n",
    "        })\n",
    "    else:\n",
    "        feature_importance_data = calculate_feature_importance(\n",
    "            model_with_weather, baseline_model,\n",
    "            X_train_full_proc, X_train_baseline_proc, \n",
    "            full_preprocessor, baseline_preprocessor,\n",
    "            split_name\n",
    "        )\n",
    "    \n",
    "    # Create metrics DataFrame\n",
    "    metrics_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save metrics for this split\n",
    "    metrics_df.to_csv(metrics_file, index=False)\n",
    "    logging.info(f\"Saved metrics for {split_name} to {metrics_file}\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    del X_train_full_proc, X_valid_full_proc, X_train_baseline_proc, X_valid_baseline_proc\n",
    "    gc.collect()\n",
    "    \n",
    "    return metrics_df, feature_importance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f9eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature_importance(model_with_weather, baseline_model, \n",
    "                                X_train_full, X_train_baseline,\n",
    "                                full_preprocessor, baseline_preprocessor,\n",
    "                                split_name):\n",
    "    \"\"\"Calculate SHAP feature importance for both models\"\"\"\n",
    "    # Check if SHAP results already exist for this split\n",
    "    shap_file = f\"{CONFIG['artifacts_dir']}{split_name}_feature_importance.csv\"\n",
    "    \n",
    "    if os.path.exists(shap_file):\n",
    "        logging.info(f\"Loading existing SHAP feature importance from {shap_file}\")\n",
    "        return pd.read_csv(shap_file)\n",
    "    \n",
    "    # Initialize feature importance data\n",
    "    feature_importance_data = {\n",
    "        \"split\": [],\n",
    "        \"model_type\": [],\n",
    "        \"feature\": [],\n",
    "        \"importance_pct\": []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Extract feature names from preprocessor for full model\n",
    "        feature_names_full = []\n",
    "        for name, transformer, columns in full_preprocessor.transformers_:\n",
    "            if name == 'low_cat' and hasattr(transformer, 'get_feature_names_out'):\n",
    "                cat_features = transformer.get_feature_names_out(columns)\n",
    "                feature_names_full.extend(cat_features)\n",
    "            elif name in ['bool', 'num', 'med_cat']:\n",
    "                feature_names_full.extend(columns)\n",
    "        \n",
    "        # Extract feature names for baseline model\n",
    "        feature_names_baseline = []\n",
    "        for name, transformer, columns in baseline_preprocessor.transformers_:\n",
    "            if name == 'low_cat' and hasattr(transformer, 'get_feature_names_out'):\n",
    "                cat_features = transformer.get_feature_names_out(columns)\n",
    "                feature_names_baseline.extend(cat_features)\n",
    "            elif name in ['bool', 'num', 'med_cat']:\n",
    "                feature_names_baseline.extend(columns)\n",
    "        \n",
    "        # Calculate SHAP values for both models\n",
    "        models_to_analyze = {\n",
    "            \"model_with_weather\": (model_with_weather, X_train_full, feature_names_full),\n",
    "            \"baseline_model\": (baseline_model, X_train_baseline, feature_names_baseline)\n",
    "        }\n",
    "        \n",
    "        for model_type, (model, X, feature_names) in models_to_analyze.items():\n",
    "            # Sample dataset to a more moderate size that balances accuracy and performance\n",
    "            # For tree-based models like LightGBM, 10,000 samples is typically sufficient\n",
    "            max_samples = 10000  # Reduced from 100,000 for better efficiency\n",
    "            \n",
    "            if X.shape[0] > max_samples:\n",
    "                logging.info(f\"Sampling {max_samples} rows from {X.shape[0]} for SHAP calculation\")\n",
    "                sample_indices = np.random.choice(X.shape[0], max_samples, replace=False)\n",
    "                X_sample = X[sample_indices]\n",
    "            else:\n",
    "                X_sample = X\n",
    "                logging.info(f\"Using all {X.shape[0]} rows for SHAP calculation\")\n",
    "            \n",
    "            # Convert sparse matrix to dense for SHAP calculation\n",
    "            if scipy.sparse.issparse(X_sample):\n",
    "                logging.info(f\"Converting sparse matrix to dense for SHAP calculation\")\n",
    "                X_sample = X_sample.toarray()\n",
    "            \n",
    "            log_step(f\"Calculating SHAP values for {model_type} with {X_sample.shape[0]} samples\")\n",
    "            \n",
    "            # Create SHAP explainer - handling both LGBMRegressor and Booster objects\n",
    "            import lightgbm as lgb\n",
    "            if isinstance(model, lgb.Booster):\n",
    "                explainer = shap.TreeExplainer(model)\n",
    "            else:\n",
    "                explainer = shap.TreeExplainer(model)\n",
    "            \n",
    "            shap_values = explainer.shap_values(X_sample)\n",
    "            \n",
    "            # Calculate feature importance\n",
    "            importance_values = np.abs(shap_values).mean(0)\n",
    "            importance_sum = importance_values.sum()\n",
    "            \n",
    "            # Convert to percentages and create DataFrame\n",
    "            if importance_sum > 0:  # Avoid division by zero\n",
    "                for i, feature_name in enumerate(feature_names[:len(importance_values)]):\n",
    "                    importance_pct = (importance_values[i] / importance_sum) * 100\n",
    "                    \n",
    "                    feature_importance_data[\"split\"].append(split_name)\n",
    "                    feature_importance_data[\"model_type\"].append(model_type)\n",
    "                    feature_importance_data[\"feature\"].append(feature_name)\n",
    "                    feature_importance_data[\"importance_pct\"].append(importance_pct)\n",
    "                    \n",
    "            # Clean up memory\n",
    "            del explainer, shap_values, X_sample\n",
    "            gc.collect()\n",
    "            log_step(f\"Completed SHAP analysis for {model_type}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calculating feature importance: {e}\")\n",
    "    \n",
    "    # Create DataFrame from the results\n",
    "    feature_importance_df = pd.DataFrame(feature_importance_data)\n",
    "    \n",
    "    # Save to file\n",
    "    if not feature_importance_df.empty:\n",
    "        feature_importance_df.to_csv(shap_file, index=False)\n",
    "        logging.info(f\"Saved SHAP feature importance to {shap_file}\")\n",
    "    \n",
    "    return feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e733616d",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b04f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all split files\n",
    "split_files = glob(CONFIG['splits_dir'] + 'sales_category*.csv')\n",
    "\n",
    "# Initialize results tracking\n",
    "all_results = []\n",
    "all_feature_importance = []\n",
    "\n",
    "# First, check for existing results and feature importance files\n",
    "for split_file in glob(CONFIG['splits_dir'] + 'sales_category*.csv'):\n",
    "    split_name = os.path.basename(split_file).replace('sales_category_', '').replace('.csv', '')\n",
    "    \n",
    "    # Check for existing feature importance files\n",
    "    shap_file = f\"{CONFIG['artifacts_dir']}{split_name}_feature_importance.csv\"\n",
    "    metrics_file = f\"{CONFIG['artifacts_dir']}{split_name}_metrics.csv\"\n",
    "    \n",
    "    # Load existing SHAP results if available\n",
    "    if os.path.exists(shap_file):\n",
    "        logging.info(f\"Found existing SHAP results for {split_name}\")\n",
    "        try:\n",
    "            shap_data = pd.read_csv(shap_file)\n",
    "            all_feature_importance.append(shap_data)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading SHAP file for {split_name}: {str(e)}\")\n",
    "    \n",
    "    # Load existing metrics if available\n",
    "    if os.path.exists(metrics_file):\n",
    "        logging.info(f\"Found existing metrics for {split_name}\")\n",
    "        try:\n",
    "            metrics_data = pd.read_csv(metrics_file)\n",
    "            all_results.append(metrics_data)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading metrics file for {split_name}: {str(e)}\")\n",
    "\n",
    "# Process each split file\n",
    "for split_file in tqdm(split_files, desc=\"Processing splits\"):\n",
    "    split_name = os.path.basename(split_file).replace('sales_category_', '').replace('.csv', '')\n",
    "    \n",
    "    # Check if we already have both metrics and SHAP results for this split\n",
    "    metrics_file = f\"{CONFIG['artifacts_dir']}{split_name}_metrics.csv\"\n",
    "    shap_file = f\"{CONFIG['artifacts_dir']}{split_name}_feature_importance.csv\"\n",
    "    \n",
    "    if os.path.exists(metrics_file) and os.path.exists(shap_file):\n",
    "        logging.info(f\"Skipping split {split_name} - results already exist\")\n",
    "        continue\n",
    "    \n",
    "    # Process each split\n",
    "    log_step(f\"Starting processing of split {split_name}\")\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = load_and_preprocess_data(split_file)\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    try:\n",
    "        split_results, feature_importance = train_and_evaluate_models(df, split_name)\n",
    "        \n",
    "        # Save split-specific metrics\n",
    "        if not split_results.empty:\n",
    "            split_results.to_csv(metrics_file, index=False)\n",
    "            logging.info(f\"Saved metrics for {split_name} to {metrics_file}\")\n",
    "        \n",
    "        all_results.append(split_results)\n",
    "        all_feature_importance.append(feature_importance)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing split {split_name}: {str(e)}\")\n",
    "        continue\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del df, split_results, feature_importance\n",
    "    gc.collect()\n",
    "    mem_mb = get_memory_usage()\n",
    "    logging.info(f'After split {split_name} completion - memory: {mem_mb} MB')\n",
    "\n",
    "# Check if we have any results\n",
    "if len(all_results) == 0:\n",
    "    logging.info(\"No results were collected. Check if all files already exist.\")\n",
    "    \n",
    "    # Try to load previously saved aggregate results\n",
    "    try:\n",
    "        results_df = pd.read_csv(f\"{CONFIG['artifacts_dir']}experiment_metrics.csv\")\n",
    "        feature_importance_df = pd.read_csv(f\"{CONFIG['artifacts_dir']}feature_importance.csv\")\n",
    "        \n",
    "        # Display previously saved results\n",
    "        print(\"\\nLoaded Previously Saved Results:\")\n",
    "        print(\"\\nPrimary Metrics:\")\n",
    "        display(results_df[['split', 'model', 'MAE', 'RMSE', 'R2', 'sMAPE', 'WAPE']])\n",
    "        \n",
    "        print(\"\\nStock Impact Metrics:\")\n",
    "        display(results_df[['split', 'model', 'over_stock_percent', 'under_stock_percent', 'Bias']])\n",
    "    except Exception as load_err:\n",
    "        logging.error(f\"Error loading previous results: {load_err}\")\n",
    "        print(\"No results were collected and no previous results could be loaded.\")\n",
    "else:\n",
    "    # Combine results\n",
    "    results_df = pd.concat(all_results)\n",
    "    feature_importance_df = pd.concat(all_feature_importance)\n",
    "\n",
    "    # Calculate averages across splits for each model, including ALL metrics\n",
    "    avg_results = results_df.groupby('model').agg({\n",
    "        'MAE': 'mean',\n",
    "        'MAPE': 'mean', \n",
    "        'sMAPE': 'mean',\n",
    "        'RMSE': 'mean',\n",
    "        'R2': 'mean',\n",
    "        'WAPE': 'mean',\n",
    "        'Bias': 'mean',\n",
    "        'over_stock': 'mean',\n",
    "        'under_stock': 'mean',\n",
    "        'over_stock_percent': 'mean',\n",
    "        'under_stock_percent': 'mean',\n",
    "        'total_sales': 'mean',\n",
    "        'train_time_sec': 'mean',\n",
    "        'rows_train': 'mean',\n",
    "        'rows_valid': 'mean'\n",
    "    }).reset_index()\n",
    "    avg_results['split'] = 'AVERAGE'\n",
    "\n",
    "    # Add averages to results\n",
    "    final_results = pd.concat([results_df, avg_results])\n",
    "\n",
    "    # Save aggregate results\n",
    "    final_results.to_csv(f\"{CONFIG['artifacts_dir']}experiment_metrics.csv\", index=False)\n",
    "\n",
    "    # Save aggregate feature importance\n",
    "    feature_importance_df.to_csv(f\"{CONFIG['artifacts_dir']}feature_importance.csv\", index=False)\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(\"\\nPrimary Metrics:\")\n",
    "    display(final_results[['split', 'model', 'MAE', 'RMSE', 'R2', 'sMAPE', 'WAPE']])\n",
    "\n",
    "    print(\"\\nStock Impact Metrics:\")\n",
    "    display(final_results[['split', 'model', 'over_stock_percent', 'under_stock_percent', 'Bias']])\n",
    "\n",
    "    print(\"\\nDetailed Metrics:\")\n",
    "    display(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c4bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total influence of weather features\n",
    "all_features = feature_importance_df[feature_importance_df['model_type'] == 'model_with_weather']\n",
    "\n",
    "# Identify weather features\n",
    "weather_features = []\n",
    "for feature in all_features['feature'].unique():\n",
    "    if any(re.search(pattern, feature) for pattern in WEATHER_PATTERNS):\n",
    "        weather_features.append(feature)\n",
    "\n",
    "# Calculate total importance of weather features\n",
    "weather_importance = all_features[all_features['feature'].isin(weather_features)]['importance_pct'].sum()\n",
    "total_importance = all_features['importance_pct'].sum()\n",
    "weather_percentage = (weather_importance / total_importance) * 100\n",
    "\n",
    "print(f\"\\nWeather Features Impact:\")\n",
    "print(f\"Weather features account for {weather_percentage:.2f}% of total feature importance in the model with weather\")\n",
    "print(f\"Number of weather features used: {len(weather_features)} out of {len(all_features['feature'].unique())} total features\")\n",
    "\n",
    "# Агрегируем важность (например, суммой) по feature\n",
    "agg_features = all_features.groupby('feature', as_index=False)['importance_pct'].mean()\n",
    "\n",
    "# Сортируем по убыванию суммарной важности\n",
    "top_weather_features = agg_features.sort_values('importance_pct', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most important weather features:\")\n",
    "display(top_weather_features[['feature', 'importance_pct']])\n",
    "\n",
    "# Сохраняем в CSV (если нужно больше — поменяй head(10) на head(10000) или сколько нужно)\n",
    "top_weather_features[['feature', 'importance_pct']].to_csv(\"results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
