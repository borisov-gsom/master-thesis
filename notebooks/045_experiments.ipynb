{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bd89327",
   "metadata": {},
   "source": [
    "# Sales Category Prediction - Incremental Learning Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b6ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import psutil\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from glob import glob\n",
    "import scipy.sparse\n",
    "\n",
    "# Feature processing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce\n",
    "\n",
    "# Models\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f74018",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b06d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Paths\n",
    "    \"splits_dir\": \"../data/splits/\",\n",
    "    \"weather_path\": \"../data/weather_processed.csv\",\n",
    "    \"artifacts_dir\": \"../artifacts/\",\n",
    "    \n",
    "    # Preprocessing\n",
    "    \"chunk_months\": 1,  # Size of sliding window in months (1 or 2)\n",
    "    \"drop_columns\": {\"group\", \"year\", \"unit\"},\n",
    "    \n",
    "    # Models\n",
    "    \"target_column\": \"qnt\",\n",
    "    \"date_column\": \"calday\",\n",
    "    \n",
    "    # Random seed\n",
    "    \"random_seed\": 42\n",
    "}\n",
    "\n",
    "# Create artifacts directory if it doesn't exist\n",
    "os.makedirs(CONFIG[\"artifacts_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a7a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature categories\n",
    "BOOL_EXPLICIT = {\"bu_exists\", \"freezing_day\", \"cold_day\", \"warm_day\", \"hot_day\"}\n",
    "BOOL_PATTERNS = [r\"^is_\", r\"^has_\", r\"_had_high_\", r\"_had_low_\", r\"_exists$\"]\n",
    "\n",
    "LOW_EXPLICIT = {\n",
    "    \"matrix_type\", \"country_id\", \"format_merch\", \"geolocal_type\",\n",
    "    \"season\", \"type_bonus_id\", \"seasonal_group\",\n",
    "    \"category_major\", \"category_detailed\", \"category_full\"\n",
    "}\n",
    "LOW_PATTERNS = [r\"^category_\"]\n",
    "\n",
    "MED_EXPLICIT = {\n",
    "    \"brand_id\", \"index_material\", \"index_store\", \"type_for_customer\",\n",
    "    \"week_iso\", \"day_of_week\", \"day_of_month\", \"month\", \"quarter\", \"source_month\"\n",
    "}\n",
    "\n",
    "NUM_PATTERNS = [\n",
    "    r\"^(temp|min|max|tempmax|tempmin|feelslike[a-z]*|dew|humidity|precip|snow|snowdepth\"\n",
    "    r\"|wind(gust|speed|dir)|sealevelpressure|cloudcover|visibility\"\n",
    "    r\"|solarradiation|solarenergy|uvindex|moonphase|daylight_hours\"\n",
    "    r\"|heat_index|temp_range)\",\n",
    "    r\".*_lag_\\\\d+d$\", r\".*_d\\\\d+to\\\\d+_(mean|min|max|std)$\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6f8072",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3142a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / 1024 / 1024\n",
    "\n",
    "def log_step(step_name):\n",
    "    \"\"\"Log step name and memory usage\"\"\"\n",
    "    mem_mb = get_memory_usage()\n",
    "    logging.info(f\"{step_name}: {mem_mb:.2f} MB\")\n",
    "    return mem_mb\n",
    "\n",
    "# Evaluation metrics\n",
    "def calculate_mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    mask_nonzero = (y_true != 0) & (y_pred != 0)\n",
    "    mask_one_zero = ((y_true == 0) & (y_pred != 0)) | ((y_true != 0) & (y_pred == 0))\n",
    "    \n",
    "    mape_nonzero = np.abs((y_true[mask_nonzero] - y_pred[mask_nonzero]) / y_true[mask_nonzero])\n",
    "    mape_one_zero = np.ones(mask_one_zero.sum())\n",
    "    \n",
    "    if len(mape_nonzero) + len(mape_one_zero) == 0:\n",
    "        return 0\n",
    "    \n",
    "    total_mape = np.concatenate([mape_nonzero, mape_one_zero]) if len(mape_one_zero) > 0 and len(mape_nonzero) > 0 else \\\n",
    "                (mape_nonzero if len(mape_nonzero) > 0 else mape_one_zero)\n",
    "    return np.mean(total_mape)\n",
    "\n",
    "def calculate_smape(y_true, y_pred):\n",
    "    mask_nonzero = (y_true != 0) & (y_pred != 0)\n",
    "    mask_one_zero = ((y_true == 0) & (y_pred != 0)) | ((y_true != 0) & (y_pred == 0))\n",
    "    \n",
    "    smape_nonzero = 2 * np.abs(y_true[mask_nonzero] - y_pred[mask_nonzero]) / (np.abs(y_true[mask_nonzero]) + np.abs(y_pred[mask_nonzero]))\n",
    "    smape_one_zero = np.ones(mask_one_zero.sum())\n",
    "    \n",
    "    if len(smape_nonzero) + len(smape_one_zero) == 0:\n",
    "        return 0\n",
    "    \n",
    "    total_smape = np.concatenate([smape_nonzero, smape_one_zero]) if len(smape_one_zero) > 0 and len(smape_nonzero) > 0 else \\\n",
    "                 (smape_nonzero if len(smape_nonzero) > 0 else smape_one_zero)\n",
    "    return np.mean(total_smape)\n",
    "\n",
    "# Additional metrics\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    \"\"\"Calculate Root Mean Squared Error\"\"\"\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "def calculate_r2(y_true, y_pred):\n",
    "    \"\"\"Calculate R-squared (coefficient of determination)\"\"\"\n",
    "    from sklearn.metrics import r2_score\n",
    "    return r2_score(y_true, y_pred)\n",
    "\n",
    "def calculate_wape(y_true, y_pred):\n",
    "    \"\"\"Calculate Weighted Absolute Percentage Error\"\"\"\n",
    "    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))\n",
    "\n",
    "def calculate_bias(y_true, y_pred):\n",
    "    \"\"\"Calculate bias (Mean Percent Forecast Error)\n",
    "    >0 → over-forecast (over-stock)\n",
    "    <0 → under-forecast (under-stock)\"\"\"\n",
    "    mask = y_true != 0\n",
    "    if not np.any(mask):\n",
    "        return 0\n",
    "    \n",
    "    percent_errors = (y_pred[mask] - y_true[mask]) / y_true[mask]\n",
    "    return np.mean(percent_errors)  # Changed from median to mean\n",
    "\n",
    "def calculate_stock_stats(y_true, y_pred, action_price):\n",
    "    \"\"\"Calculate over-stock and under-stock statistics\"\"\"\n",
    "    differences = y_pred - y_true\n",
    "    over_stock = np.where(differences > 0, differences * action_price, 0)\n",
    "    under_stock = np.where(differences < 0, -differences * action_price, 0)\n",
    "    \n",
    "    total_sales = np.sum(y_true * action_price)\n",
    "    over_stock_sum = np.sum(over_stock)\n",
    "    under_stock_sum = np.sum(under_stock)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if total_sales == 0:\n",
    "        over_stock_percent = 0\n",
    "        under_stock_percent = 0\n",
    "    else:\n",
    "        over_stock_percent = over_stock_sum / total_sales * 100\n",
    "        under_stock_percent = under_stock_sum / total_sales * 100\n",
    "    \n",
    "    return {\n",
    "        'total_sales': total_sales,\n",
    "        'over_stock': over_stock_sum,\n",
    "        'under_stock': under_stock_sum,\n",
    "        'over_stock_percent': over_stock_percent,\n",
    "        'under_stock_percent': under_stock_percent\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8093be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_predictions(y_pred, qnt_max):\n",
    "    \"\"\"Apply post-processing rules to prediction\"\"\"\n",
    "    # Clip negative values to 0\n",
    "    y_pred = np.clip(y_pred, 0, None)\n",
    "    \n",
    "    # Apply outlier correction where needed\n",
    "    mask = (qnt_max >= 5) & (y_pred > 2 * qnt_max)\n",
    "    if np.any(mask):\n",
    "        y_pred[mask] = 2 * qnt_max[mask]\n",
    "    \n",
    "    # Round to nearest integer\n",
    "    y_pred = np.round(y_pred).astype(int)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e04e22",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c818de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_types(df):\n",
    "    \"\"\"Identify feature types based on rules\"\"\"\n",
    "    columns = df.columns.tolist()\n",
    "    feature_types = {'bool_cols': [], 'low_cat_cols': [], 'med_cat_cols': [], 'num_cols': []}\n",
    "    \n",
    "    # Find target and ID columns\n",
    "    target_col = CONFIG['target_column']\n",
    "    date_col = CONFIG['date_column']\n",
    "    drop_cols = list(CONFIG['drop_columns']) + [target_col, date_col] if target_col in columns and date_col in columns else []\n",
    "    \n",
    "    # Collect columns by type\n",
    "    for col in columns:\n",
    "        if col in drop_cols:\n",
    "            continue\n",
    "        \n",
    "        # Check bool columns\n",
    "        if col in BOOL_EXPLICIT or any(re.match(pattern, col) for pattern in BOOL_PATTERNS) or df[col].dtype == bool:\n",
    "            feature_types['bool_cols'].append(col)\n",
    "            continue\n",
    "            \n",
    "        # Check low cardinality categorical\n",
    "        if col in LOW_EXPLICIT or any(re.match(pattern, col) for pattern in LOW_PATTERNS):\n",
    "            feature_types['low_cat_cols'].append(col)\n",
    "            continue\n",
    "            \n",
    "        # Check medium cardinality categorical\n",
    "        if col in MED_EXPLICIT:\n",
    "            feature_types['med_cat_cols'].append(col)\n",
    "            continue\n",
    "            \n",
    "        # Check numeric by pattern\n",
    "        if any(re.match(pattern, col) for pattern in NUM_PATTERNS) or df[col].dtype in ['int32', 'int64', 'float32', 'float64']:\n",
    "            feature_types['num_cols'].append(col)\n",
    "    \n",
    "    return feature_types\n",
    "\n",
    "def create_column_transformer(df):\n",
    "    \"\"\"Create a ColumnTransformer for preprocessing\"\"\"\n",
    "    feature_types = get_feature_types(df)\n",
    "    \n",
    "    # Log the number of features of each type for debugging\n",
    "    logging.info(f\"Feature counts: bool={len(feature_types['bool_cols'])}, low_cat={len(feature_types['low_cat_cols'])}, \"\n",
    "                 f\"med_cat={len(feature_types['med_cat_cols'])}, num={len(feature_types['num_cols'])}\")\n",
    "    \n",
    "    transformers = []\n",
    "    \n",
    "    # Boolean columns to uint8\n",
    "    if feature_types['bool_cols']:\n",
    "        transformers.append(('bool', 'passthrough', feature_types['bool_cols']))\n",
    "    \n",
    "    # Low cardinality categorical columns to one-hot\n",
    "    if feature_types['low_cat_cols']:\n",
    "        transformers.append(('low_cat', \n",
    "                            OneHotEncoder(sparse_output=True, handle_unknown='ignore', dtype=np.int8),\n",
    "                            feature_types['low_cat_cols']))\n",
    "    \n",
    "    # Medium cardinality categorical columns with count encoding\n",
    "    if feature_types['med_cat_cols']:\n",
    "        # First check if we have any medium cardinality columns\n",
    "        if len(feature_types['med_cat_cols']) > 0:\n",
    "            # Create a copy of the DataFrame with only the medium cardinality columns\n",
    "            med_cat_df = df[feature_types['med_cat_cols']].copy()\n",
    "            \n",
    "            # Convert all columns to categorical type\n",
    "            for col in med_cat_df.columns:\n",
    "                med_cat_df[col] = med_cat_df[col].astype('category')\n",
    "            transformers.append(('med_cat', \n",
    "                              ce.CountEncoder(normalize=True), \n",
    "                              feature_types['med_cat_cols']))\n",
    "    \n",
    "    # Numeric columns - just pass through, no imputation needed\n",
    "    if feature_types['num_cols']:\n",
    "        transformers.append(('num', \n",
    "                            'passthrough', \n",
    "                            feature_types['num_cols']))\n",
    "    \n",
    "    return ColumnTransformer(transformers, remainder='drop', n_jobs=1)  # Use single thread to reduce memory\n",
    "\n",
    "def cast_types(df):\n",
    "    \"\"\"Downcast types for efficiency\"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "        elif df[col].dtype == 'int64':\n",
    "            df[col] = df[col].astype('int32')\n",
    "        elif df[col].dtype == 'bool':\n",
    "            df[col] = df[col].astype('uint8')\n",
    "    return df\n",
    "    \n",
    "def load_and_preprocess_data(split_file):\n",
    "    \"\"\"Load and preprocess a split file\"\"\"\n",
    "    log_step(f\"Loading split file {os.path.basename(split_file)}\")\n",
    "    \n",
    "    # Read the split file\n",
    "    df = pd.read_csv(split_file)\n",
    "    \n",
    "    # Read weather data\n",
    "    weather_df = pd.read_csv(CONFIG['weather_path'])\n",
    "    \n",
    "    # Merge with weather data\n",
    "    df = pd.merge(df, weather_df, on=CONFIG['date_column'], how='left')\n",
    "    \n",
    "    # Cast types\n",
    "    df = cast_types(df)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    for col in CONFIG['drop_columns']:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    # Sort by date\n",
    "    df.sort_values(by=CONFIG['date_column'], inplace=True)\n",
    "    \n",
    "    # Convert date to datetime\n",
    "    if df[CONFIG['date_column']].dtype == 'object':\n",
    "        df[CONFIG['date_column']] = pd.to_datetime(df[CONFIG['date_column']])\n",
    "    \n",
    "    # Explicitly convert medium cardinality columns to category without logging each conversion\n",
    "    for col in MED_EXPLICIT:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    # Calculate max_qnt for post-processing only if it doesn't exist\n",
    "    if 'qnt_max' not in df.columns:\n",
    "        logging.info(f\"'qnt_max' column not found, calculating it now\")\n",
    "        df['qnt_max'] = df.groupby(['brand_id', 'country_id', 'category_detailed'])[CONFIG['target_column']].transform('max')\n",
    "    \n",
    "    # Log transformation for specific quantity columns\n",
    "    log_columns = [\n",
    "        'qnt', 'qnt_loss', 'qnt_lag_14d', 'qnt_lag_21d', 'qnt_lag_28d', \n",
    "        'qnt_lag_avg', 'qnt_max', 'qnt_min', 'qnt_mean', 'qnt_median'\n",
    "    ]\n",
    "    \n",
    "    # Store original values before transformation\n",
    "    for col in log_columns:\n",
    "        if col in df.columns:\n",
    "            df[f\"{col}_orig\"] = df[col].copy()\n",
    "    \n",
    "    # Apply log1p transformation\n",
    "    for col in log_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = np.log1p(df[col])\n",
    "    \n",
    "    log_step(f\"Preprocessed split shape: {df.shape}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df8183",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models():\n",
    "    \"\"\"Create models with optimized GPU acceleration parameters for fair comparison\"\"\"\n",
    "    models = {\n",
    "        \"XGBoost\": XGBRegressor(\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=8,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            tree_method=\"gpu_hist\",  # GPU acceleration\n",
    "            predictor=\"gpu_predictor\",\n",
    "            max_bin=256,  # Optimize for limited VRAM\n",
    "            objective=\"reg:squarederror\",\n",
    "            eval_metric=\"mae\",  # Consistent metric across models\n",
    "            n_jobs=-1,\n",
    "            random_state=CONFIG[\"random_seed\"],\n",
    "            early_stopping_rounds=200  # Increased for better convergence\n",
    "        ),\n",
    "        \"LightGBM\": LGBMRegressor(\n",
    "            device_type=\"gpu\",  # GPU acceleration\n",
    "            boosting_type=\"gbdt\",\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=255,  # Equivalent to max_depth=8\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            early_stopping_round=200,  # Correct parameter name (without 's')\n",
    "            metric=\"mae\",  # Consistent metric across models\n",
    "            n_jobs=1,      # Use 1 for GPU mode\n",
    "            gpu_platform_id=0,  # First OpenCL platform\n",
    "            gpu_device_id=0,    # First GPU device\n",
    "            verbose=-1,         # Progress info without too many messages\n",
    "            random_state=CONFIG[\"random_seed\"]\n",
    "        ),\n",
    "        \"CatBoost\": CatBoostRegressor(\n",
    "            iterations=10000,\n",
    "            depth=8,\n",
    "            learning_rate=0.05,\n",
    "            loss_function=\"RMSE\",  # Use RMSE for GPU compatibility\n",
    "            # Removed eval_metric=\"MAE\" as it's not GPU compatible\n",
    "            task_type=\"GPU\",\n",
    "            devices=\"0\",  # Explicitly use first GPU\n",
    "            random_seed=CONFIG[\"random_seed\"],\n",
    "            verbose=100,  # Show progress every 100 iterations\n",
    "            early_stopping_rounds=200,\n",
    "            allow_writing_files=False  # Save time by not writing snapshots\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1709229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(df, split_name):\n",
    "    \"\"\"Train models on the entire split without incremental learning\"\"\"\n",
    "    log_step(f\"Starting model training for split {split_name}\")\n",
    "    \n",
    "    # Define target and date columns\n",
    "    target_col = CONFIG['target_column']  # 'qnt' (log-transformed)\n",
    "    target_orig_col = f\"{target_col}_orig\"  # Original target for evaluation\n",
    "    date_col = CONFIG['date_column']\n",
    "    qnt_max_col = 'qnt_max'\n",
    "    qnt_max_orig_col = 'qnt_max_orig'\n",
    "    \n",
    "    # Exclude target column and date column from features\n",
    "    feature_cols = [col for col in df.columns if col not in [target_col, target_orig_col, date_col] and not col.endswith('_orig')]\n",
    "    \n",
    "    # Create preprocessor\n",
    "    preprocessor = create_column_transformer(df[feature_cols])\n",
    "    \n",
    "    # Create split for training (80%) and validation (20%)\n",
    "    df_sorted = df.sort_values(by=date_col)\n",
    "    split_idx = int(len(df_sorted) * 0.8)\n",
    "    \n",
    "    train_df = df_sorted.iloc[:split_idx]\n",
    "    valid_df = df_sorted.iloc[split_idx:]\n",
    "    \n",
    "    # Extract features and target\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[target_col]  # Log-transformed target\n",
    "    \n",
    "    X_valid = valid_df[feature_cols]\n",
    "    y_valid = valid_df[target_col]  # Log-transformed target\n",
    "    y_valid_orig = valid_df[target_orig_col]  # Original target for evaluation\n",
    "    qnt_max_valid = valid_df[qnt_max_col]\n",
    "    qnt_max_valid_orig = valid_df[qnt_max_orig_col]\n",
    "    \n",
    "    # Get action prices for stock statistics\n",
    "    action_price = valid_df['action_price'] if 'action_price' in valid_df.columns else np.ones(len(valid_df))\n",
    "    \n",
    "    log_step(f\"Train size: {len(X_train)}, Validation size: {len(X_valid)}\")\n",
    "    \n",
    "    # Preprocess features\n",
    "    X_train_proc = preprocessor.fit_transform(X_train)\n",
    "    X_valid_proc = preprocessor.transform(X_valid)\n",
    "    \n",
    "    # Clean up memory\n",
    "    del X_train, X_valid\n",
    "    gc.collect()\n",
    "    mem_mb = get_memory_usage()\n",
    "    logging.info(f'After preprocessing, memory usage: {mem_mb:.2f} MB')\n",
    "    \n",
    "    # Create models\n",
    "    models = create_models()\n",
    "    \n",
    "    # Results dictionary with all metrics\n",
    "    results = {\n",
    "        \"split\": [],\n",
    "        \"model\": [],\n",
    "        \"MAE\": [],\n",
    "        \"MAPE\": [],\n",
    "        \"sMAPE\": [],\n",
    "        \"RMSE\": [],\n",
    "        \"R2\": [],\n",
    "        \"WAPE\": [],\n",
    "        \"Bias\": [],\n",
    "        \"over_stock\": [],\n",
    "        \"under_stock\": [],\n",
    "        \"over_stock_percent\": [],\n",
    "        \"under_stock_percent\": [],\n",
    "        \"total_sales\": [],\n",
    "        \"train_time_sec\": [],\n",
    "        \"rows_train\": [],\n",
    "        \"rows_valid\": []\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for model_name, model in models.items():\n",
    "        log_step(f\"Training {model_name}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if model_name == \"CatBoost\":\n",
    "            # Import Pool for CatBoost\n",
    "            from catboost import Pool\n",
    "            \n",
    "            # For CatBoost, we need dense matrices\n",
    "            if scipy.sparse.issparse(X_train_proc):\n",
    "                logging.info(f\"Converting sparse matrix to dense for CatBoost GPU training\")\n",
    "                X_train_dense = X_train_proc.toarray().astype(np.float32)\n",
    "                X_valid_dense = X_valid_proc.toarray().astype(np.float32)\n",
    "            else:\n",
    "                X_train_dense = X_train_proc\n",
    "                X_valid_dense = X_valid_proc\n",
    "            \n",
    "            # Create Pool objects for CatBoost (recommended for better performance)\n",
    "            train_pool = Pool(X_train_dense, y_train)\n",
    "            valid_pool = Pool(X_valid_dense, y_valid)\n",
    "            \n",
    "            # Train CatBoost using Pool objects\n",
    "            logging.info(f\"Starting CatBoost training with GPU acceleration...\")\n",
    "            model.fit(\n",
    "                train_pool, \n",
    "                eval_set=valid_pool,\n",
    "                use_best_model=True,\n",
    "                verbose=100  # Show progress every 100 iterations\n",
    "            )\n",
    "            \n",
    "            # Clean up temporary objects\n",
    "            del train_pool, valid_pool\n",
    "            if scipy.sparse.issparse(X_train_proc):\n",
    "                del X_train_dense, X_valid_dense\n",
    "            gc.collect()\n",
    "            \n",
    "            # Predict - need to convert to dense for prediction if sparse\n",
    "            if scipy.sparse.issparse(X_valid_proc):\n",
    "                y_pred_log = model.predict(X_valid_proc.toarray().astype(np.float32))\n",
    "            else:\n",
    "                y_pred_log = model.predict(X_valid_proc)\n",
    "                \n",
    "        elif model_name == \"XGBoost\":\n",
    "            # Train XGBoost with consistent API\n",
    "            logging.info(f\"Starting XGBoost training with GPU acceleration...\")\n",
    "            model.fit(\n",
    "                X_train_proc, y_train,\n",
    "                eval_set=[(X_valid_proc, y_valid)],\n",
    "                verbose=100  # Show evaluation every 100 iterations\n",
    "            )\n",
    "            y_pred_log = model.predict(X_valid_proc)\n",
    "            \n",
    "        elif model_name == \"LightGBM\":\n",
    "            # Log LightGBM GPU status without lgb.basic\n",
    "            import lightgbm as lgb\n",
    "            logging.info(f\"LightGBM version: {lgb.__version__}\")\n",
    "            \n",
    "            # Just log GPU settings from the model parameters\n",
    "            logging.info(f\"LightGBM GPU settings: device_type={model.device_type}, \"\n",
    "                         f\"gpu_platform_id={model.gpu_platform_id}, gpu_device_id={model.gpu_device_id}\")\n",
    "            \n",
    "            # Train LightGBM with consistent API and progress logging\n",
    "            logging.info(f\"Starting LightGBM training with GPU acceleration...\")\n",
    "            model.fit(\n",
    "                X_train_proc, y_train,\n",
    "                eval_set=[(X_valid_proc, y_valid)],\n",
    "                callbacks=[lgb.log_evaluation(period=100)]  # Log every 100 iterations\n",
    "            )\n",
    "            y_pred_log = model.predict(X_valid_proc)\n",
    "        \n",
    "        # Verify log-scale prediction values\n",
    "        logging.info(f\"{model_name} - Log-scale predictions: min={y_pred_log.min():.4f}, max={y_pred_log.max():.4f}, mean={y_pred_log.mean():.4f}\")\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        y_pred = np.expm1(y_pred_log)\n",
    "        \n",
    "        # Apply post-processing (includes rounding to integers) using original-scale qnt_max\n",
    "        y_pred = postprocess_predictions(y_pred, qnt_max_valid_orig.values)\n",
    "        \n",
    "        # Verify original-scale predictions after postprocessing\n",
    "        logging.info(f\"{model_name} - Original scale predictions: min={y_pred.min():.4f}, max={y_pred.max():.4f}, mean={y_pred.mean():.4f}\")\n",
    "        \n",
    "        # Calculate metrics using original-scale values\n",
    "        mae = calculate_mae(y_valid_orig.values, y_pred)\n",
    "        mape = calculate_mape(y_valid_orig.values, y_pred)\n",
    "        smape = calculate_smape(y_valid_orig.values, y_pred)\n",
    "        \n",
    "        # Calculate additional metrics on original scale\n",
    "        rmse = calculate_rmse(y_valid_orig.values, y_pred)\n",
    "        r2 = calculate_r2(y_valid_orig.values, y_pred)\n",
    "        wape = calculate_wape(y_valid_orig.values, y_pred)\n",
    "        bias = calculate_bias(y_valid_orig.values, y_pred)\n",
    "        \n",
    "        # Calculate stock statistics on original scale\n",
    "        stock_stats = calculate_stock_stats(y_valid_orig.values, y_pred, action_price)\n",
    "        \n",
    "        # Record training time\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Log detailed results\n",
    "        logging.info(f\"{model_name} - Training time: {train_time:.2f}s\")\n",
    "        logging.info(f\"{model_name} - MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "        logging.info(f\"{model_name} - MAPE: {mape:.4f}, sMAPE: {smape:.4f}, WAPE: {wape:.4f}, Bias: {bias:.4f}\")\n",
    "        logging.info(f\"{model_name} - Over-stock: {stock_stats['over_stock']:.2f} ({stock_stats['over_stock_percent']:.2f}%), Under-stock: {stock_stats['under_stock']:.2f} ({stock_stats['under_stock_percent']:.2f}%)\")\n",
    "        \n",
    "        # Save results\n",
    "        results[\"split\"].append(split_name)\n",
    "        results[\"model\"].append(model_name)\n",
    "        results[\"MAE\"].append(mae)\n",
    "        results[\"MAPE\"].append(mape)\n",
    "        results[\"sMAPE\"].append(smape)\n",
    "        results[\"RMSE\"].append(rmse)\n",
    "        results[\"R2\"].append(r2)\n",
    "        results[\"WAPE\"].append(wape)\n",
    "        results[\"Bias\"].append(bias)\n",
    "        results[\"over_stock\"].append(stock_stats['over_stock'])\n",
    "        results[\"under_stock\"].append(stock_stats['under_stock'])\n",
    "        results[\"over_stock_percent\"].append(stock_stats['over_stock_percent'])\n",
    "        results[\"under_stock_percent\"].append(stock_stats['under_stock_percent'])\n",
    "        results[\"total_sales\"].append(stock_stats['total_sales'])\n",
    "        results[\"train_time_sec\"].append(train_time)\n",
    "        results[\"rows_train\"].append(len(y_train))\n",
    "        results[\"rows_valid\"].append(len(y_valid))\n",
    "        \n",
    "        # Save model\n",
    "        model_filename = f\"{CONFIG['artifacts_dir']}{split_name}_{model_name}.model\"\n",
    "        if model_name == \"XGBoost\":\n",
    "            model.save_model(model_filename)\n",
    "        elif model_name == \"LightGBM\":\n",
    "            model.booster_.save_model(model_filename)\n",
    "        elif model_name == \"CatBoost\":\n",
    "            model.save_model(model_filename)\n",
    "    \n",
    "    # Clean up memory\n",
    "    del y_train, y_valid, y_valid_orig, X_train_proc, X_valid_proc\n",
    "    gc.collect()\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e0502",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d241777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find all split files\n",
    "split_files = glob(CONFIG['splits_dir'] + 'sales_category_*.csv')\n",
    "\n",
    "# Initialize results tracking\n",
    "all_results = []\n",
    "\n",
    "# Process each split file\n",
    "for split_file in tqdm(split_files, desc=\"Processing splits\"):\n",
    "    split_name = os.path.basename(split_file).replace('sales_category_', '').replace('.csv', '')\n",
    "    \n",
    "    log_step(f\"Starting processing of split {split_name}\")\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = load_and_preprocess_data(split_file)\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    split_results = train_and_evaluate_models(df, split_name)\n",
    "    all_results.append(split_results)\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del df, split_results\n",
    "    gc.collect()\n",
    "    mem_mb = get_memory_usage()\n",
    "    logging.info(f'After split {split_name} completion - memory: {mem_mb} MB')\n",
    "\n",
    "# Combine results\n",
    "results_df = pd.concat(all_results)\n",
    "\n",
    "# Calculate averages across splits for each model, including ALL metrics\n",
    "avg_results = results_df.groupby('model').agg({\n",
    "    'MAE': 'mean',\n",
    "    'MAPE': 'mean', \n",
    "    'sMAPE': 'mean',\n",
    "    'RMSE': 'mean',\n",
    "    'R2': 'mean',\n",
    "    'WAPE': 'mean',\n",
    "    'Bias': 'mean',\n",
    "    'over_stock': 'mean',\n",
    "    'under_stock': 'mean',\n",
    "    'over_stock_percent': 'mean',\n",
    "    'under_stock_percent': 'mean',\n",
    "    'total_sales': 'mean',\n",
    "    'train_time_sec': 'mean',\n",
    "    'rows_train': 'mean',\n",
    "    'rows_valid': 'mean'\n",
    "}).reset_index()\n",
    "avg_results['split'] = 'AVERAGE'\n",
    "\n",
    "# Add averages to results\n",
    "final_results = pd.concat([results_df, avg_results])\n",
    "\n",
    "# Save results\n",
    "final_results.to_csv(f\"{CONFIG['artifacts_dir']}experiment_metrics.csv\", index=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nFinal Results:\")\n",
    "print(\"\\nPrimary Metrics:\")\n",
    "display(final_results[['split', 'model', 'MAE', 'RMSE', 'R2', 'sMAPE', 'WAPE']])\n",
    "\n",
    "print(\"\\nStock Impact Metrics:\")\n",
    "display(final_results[['split', 'model', 'over_stock_percent', 'under_stock_percent', 'Bias']])\n",
    "\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "display(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b361bff5-03c6-4bed-ac51-d3f41436ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca4bb27-8e88-49c5-8fec-6916c5bd77f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFinal Results:\")\n",
    "print(\"\\nPrimary Metrics:\")\n",
    "display(final_results[['split', 'model', 'MAE', 'RMSE', 'R2', 'sMAPE', 'WAPE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c1675c-69a3-45e1-beb5-770e658f8eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStock Impact Metrics:\")\n",
    "display(final_results[['split', 'model', 'over_stock_percent', 'under_stock_percent', 'Bias']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee3db42-30c3-4ad0-b79a-4cef9886562a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
